% Autogenerated translation of doc.md by Texpad
% To stop this file being overwritten during the typeset process, please move or remove this header

\documentclass[12pt]{report}
\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,left=1in,right=1in,top=1in,bottom=1in]{geometry}
\setlength\parindent{0pt}
\setlength{\parskip}{\baselineskip}
\renewcommand*\familydefault{\sfdefault}
\usepackage{hyperref}
\pagestyle{plain}
\usepackage{standalone}

\usepackage{times}
\usepackage{float}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}  

\newcommand{\ti}{\tilde}
\begin{document}

\title{Documentation: \\
Policy synthesis via formal abstraction}
\maketitle

\tableofcontents
\chapter{Do abstraction of LTI system}
\section{Computation of simulation relation}
Define LTI system as 
 \begin{align} \label{eq:LTI} \begin{aligned}
x_{k+1}&=A x_{k} + B u_k+ w_k, \qquad w_k \sim\mathcal N(0,\Sigma)\\
y_k&=Cx_k \end{aligned} \end{align}
with 
\begin{itemize}
	\item $x$ state of size $n$
	\item $u$ input of size $m$
	\item $A$ matrix of size $n\times n$
	\item $B$ matrix of size $n\times m$
	\item $y$ the output (used to compare accuracy)
	\item $C$ output matrix of size $q\times n$
	\item $\Sigma$ diagonal matrix
\end{itemize}



These stochastic transitions \eqref{eq:LTI} can  be  abstracted to a finite state model  with states $s\in S=1,2,..., $. Each state $s$ is associated to a 
representative point $x_s \in \mathbb{R}^n$ and associated to a 
cell $\Delta_s=\{x_s\} \oplus \prod_i^n [-d_i, d_i]$.
Further it  has transitions 
\begin{align}\label{eq:tgrid}
t_{grid}(s'|s,u)=\hat t \left(\Delta_{s'}\mid x_s, u\right)
\end{align}where $\hat t$ is the stochastic transition kernel associated with  \eqref{eq:LTI}.

As written in the paper, the difference between the concrete and abstract system evolves over time as follows
\begin{align}\label{eq:transition}
x_+- \tilde x_{+}=(A+BK)(x- \tilde x)+\mathbf r 
\end{align}
with \begin{itemize}
	\item $\mathbf r$ in a polytope, i.e., $\mathbf r\in \mathcal V(r_i) $, the polytope generated from vertices $r_i$.
\end{itemize}

Consider a set defined as 
\begin{align}
\mathcal R:= \{(\tilde x,x)\mid (x- \tilde x)^TM (x- \tilde x)\leq \epsilon^2\}	
\end{align} 

\subsection{Optimize $\mathcal R$ for given grid $d_1,d_2,\ldots, d_3$}
\noindent\textbf{Objective:} Design $M$, $K$ and $\epsilon$ such that 
if $(\tilde x,x)\in\mathcal R$ then also 
\[\{(x_+- \tilde x_{+})| \mbox{ s.t. \eqref{eq:transition} }\forall \mathbf r\in \mathcal V(r_i)\}\subseteq \mathcal R .\]
More over for all $(\tilde x,x)\in \mathcal R$ it should hold that $d(\tilde y,y)\leq \epsilon$. The latter can be expressed as $C^TC\preceq M$.
The former can be written with matrix  inequalities as
\begin{align*}
	(x_+- \tilde x_+)^TM (x_+- \tilde x_+)\leq \epsilon^2
\\
	((A+BK)(x- \tilde x)+\mathbf r )^TM ((A+BK)(x- \tilde x)+\mathbf r )\leq \epsilon^2	
\end{align*}

Hence we get something of this form
\[ (x- \tilde x)^TM (x- \tilde x)\leq \epsilon^2\implies 	((A+BK)(x- \tilde x)+\mathbf r )^TM ((A+BK)(x- \tilde x)+\mathbf r )\leq \epsilon^2	\]

\fbox{
\noindent\begin{minipage}[b]{.8\textwidth}
	\noindent\textbf{S-procedure}\footnote{\url{https://en.wikipedia.org/wiki/S-procedure}}\\
The implications
	\begin{align}
		x^T F_1 x+ 2g_1^Tx+h_1\leq 0\implies x^T F_2 x+ 2g_2^Tx+h_2\leq 0
	\end{align}
	holds if and only if there exists $\lambda\geq 0$ such that
	\begin{align}
		\lambda \begin{bmatrix}
		F_1& g_1\\g_1^T & h_1	
		\end{bmatrix}-\begin{bmatrix}
		F_2& g_2\\g_2^T & h_2
		\end{bmatrix} \succeq 0
	\end{align}
%	This is equivalent to $\beta\geq 0$ 
%		\begin{align}
%		 \begin{bmatrix}
%		F_1& g_1\\g_1^T & h_1	
%		\end{bmatrix}-\beta\begin{bmatrix}
%		F_2& g_2\\g_2^T & h_2
%		\end{bmatrix} \succeq 0
%	\end{align}
\end{minipage}
}


Using the S-procedure  we get
\begin{align}	
(x- \tilde x)^T (A+BK)^TM (A+BK)(x- \tilde x)
+2 \mathbf r^T M(A+BK)(x- \tilde x) +  \mathbf r^T M\mathbf r
%
\leq \epsilon^2	\\
	\lambda\begin{bmatrix}
		M&0\\0&-\epsilon^2
	\end{bmatrix}-  \begin{bmatrix}
		(A+BK)^TM (A+BK) &(A+BK)^T M  \mathbf r\\ \mathbf r^T M(A+BK)& \mathbf r^T M\mathbf r-\epsilon^2
	\end{bmatrix} \succeq 0
	\\
	\begin{bmatrix}
		\lambda M- ((A+BK)^TM (A+BK))&- (A+BK)^T M  \mathbf r\\-   \mathbf r^T M(A+BK) &(1-\lambda )\epsilon^2-  \mathbf r^T M\mathbf  r
	\end{bmatrix} \succeq 0\\
		\begin{bmatrix}
		\lambda M&0 \\0 &(1-\lambda )\epsilon^2
	\end{bmatrix} -	\begin{bmatrix}
	  ((A+BK)^TM (A+BK))& (A+BK)^T M  \mathbf r\\   \mathbf r^T M(A+BK) &  \mathbf r^T M\mathbf  r
	\end{bmatrix}  \succeq 0\\
		\begin{bmatrix}
		\lambda M&0 \\0 &(1-\lambda )\epsilon^2
	\end{bmatrix} - 	\begin{bmatrix}
 		  (A+BK)^TM\\\mathbf r^T M
 	\end{bmatrix}M^{-1}
	\begin{bmatrix}
 		  (A+BK)^TM\\\mathbf r^T M
 	\end{bmatrix}^T \succeq 0\\
 	\begin{bmatrix}
 		\lambda M&0 				& 	(A+BK)^TM		\\
 		0 &(1-\lambda )\epsilon^2& \mathbf r^T M	\\
 		M (A+BK)& M \mathbf r& M
 	\end{bmatrix} \succeq 0\\
 		\begin{bmatrix}
 		\lambda M^{-1}&0 				& M^{-1}	(A+BK)^T 		\\
 		0 &(1-\lambda )\epsilon^2& \mathbf r^T  	\\
 	  (A+BK)M^{-1}& \mathbf r& M^{-1} 
 	\end{bmatrix} \succeq 0\\
 		\begin{bmatrix}
 		\lambda M^{-1}&0 				& M^{-1}	(A+BK)^T 		\\
 		0 &(1-\lambda )\epsilon^2&  r_i^T  	\\
 	  (A+BK)M^{-1}&  r_i & M^{-1} 
 	\end{bmatrix} \succeq 0,\  \forall r_i \\ 
\end{align}
Remark that this implies that $1-\lambda \geq0$ hence $1\geq\lambda\geq0 $.  And remark that 

The objective to find a minimal $\epsilon$ can be expressed as follows
\begin{align}
\mathbf{Objective:    }& \min_{M_{inv}, L} \epsilon^2\\
&	\begin{bmatrix}
 		\lambda M_{inv}&0 				& 	M_{inv}A^T+L^TB^T 		\\
 		0 &(1-\lambda )\epsilon^2&   r_i^T  	\\
 	  AM_{inv}+BL& r_i& M_{inv} \\
 	\end{bmatrix}\succeq0\\
&  \begin{bmatrix}
  	M_{inv} & M_{inv}C^T\\
  	CM_{inv} & I
  \end{bmatrix} \succeq0
\end{align}
with $LM=K$ and $M^{-1}=M_{inv}$.
This has been implemented as function $eps\_err()$ in python.

  \subsubsection*{Verify that Polytope $\mathcal V(r_i)$ is in relation. }
 
$\mathcal V(r_i)$ is in relation $
\mathcal R:= \{(\tilde x,x)\mid (x- \tilde x)^TM (x- \tilde x)\leq \epsilon^2\}	
$ if  for all $r_i$ it holds that\[r_i^TMr_i\leq \epsilon^2 \].

\subsubsection*{Plot simulation relation}
Input  
\[\mathcal R:= \{x\mid x^TM_{\epsilon} x \leq 1\}\]
 \begin{enumerate}
	\item Compute $M_\epsilon^{1/2}=U\Sigma^{1/2}$ with singular value decomposition $M_\epsilon=U\Sigma V^T$ 
	\item Switch variable
	\[\mathcal R:= \{(\tilde x,x)\mid z^T  z \leq 1 \mbox { with } z =M_{\epsilon}^{1/2}x \}\]
	\item compute outline given angle $\alpha$
	\[z(\alpha) = \begin{bmatrix}
		\cos(\alpha)\\ \sin(\alpha)
	\end{bmatrix} 
	\] remark $z^Tz=1$.
	then $x(\alpha)=\Sigma^{-1/2} U^Tz(\alpha) $.

\end{enumerate}
 
 
 \subsection{Optimise gridding for 2d models}
 For 2 D models a routine {\it tune\_dratio} finds the optimal gridding ratio.
 
 
 
 \section{ LTI with different noise sources}
 
 Define the {\bf concrete} LTI system as 
 \begin{align} \label{eq:LTI} \begin{aligned}
x_{k+1}&=A x_{k} + B u_k+ w_k \qquad w_k \sim\mathcal N(0,\Sigma)\\
y_k&=Cx_k \end{aligned} \end{align}
with 
\begin{enumerate*} 
	\item $x$ state of size $n$
	\item $u$ input of size $m$
	\item $A$ matrix of size $n\times n$
	\item $B$ matrix of size $n\times m$
	\item $y$ the output (used to compare accuracy)
	\item $C$ output matrix of size $q\times n$
%	\item $D$ matrix currently assumed to be zero
\end{enumerate*}
 Suppose that an  {\bf abstract} LTI system has been given as  
 \begin{align} \label{eq:LTI} \begin{aligned}
\ti x_{k+1}&=A \ti x_{k} + B \ti u_k+ \ti w_k \qquad \ti w_k \sim\mathcal N(0,\Sigma)\\
y_k&=Cx_k \end{aligned} \end{align}

To lift the two systems, we consider the existence of the following combined system 
 \begin{align} \label{eq:LTI} \begin{aligned}
 x_{k+1}&=A x_{k} + B u_k+ B_w s_k \\
\ti x_{k+1}&=A \ti x_{k} + B \ti u_k+ \ti B_w s_k \qquad s_k \sim\mathcal N(0,\Sigma)\end{aligned} \end{align}
Given $u_k=\tilde u_k + K(x_k-\tilde x_k)$, the state difference evolves as 
 \begin{align} \label{eq:LTI} \begin{aligned}
 x_{k+1}-\ti x_{k+1}&=(A+BK) (x_{k}- \ti x_{k}) + (B_w -  \ti B_w) s_k \\
 &\qquad s_k \sim\mathcal N(0,\Sigma)\end{aligned} \end{align} 
 
 Given that $\Sigma$ is a diagonal matrix, we can easily compute the probability that $s_k$ is in a hypercube, i.e., $s_k\in\prod_i [ -a_i, a_i ]$,  with associated probability $1-\delta = \prod_i\mathcal N([ -a_i, a_i ]|0,\sigma_i)$.
 Hence to quantify a simulation relation we consider the $1-\delta$  invariance of transitions 
  \begin{align} \label{eq:LTI} \begin{aligned}
 x_{k+1}-\ti x_{k+1}&=(A+BK) (x_{k}- \ti x_{k}) + (B_w -  \ti B_w) \mathbf s 
 \quad \forall  \mathbf  s\in\prod_i [ -a_i, a_i ] \end{aligned} \end{align} 
 
 
 In combination with the gridding we get that 
   \begin{align} \label{eq:LTI} \begin{aligned}
 x_{k+1}-\ti x_{k+1}&=(A+BK) (x_{k}- \ti x_{k}) + (B_w -  \ti B_w) \mathbf s  + \mathbf r
 \\&\quad \forall  \mathbf  s\in\prod_i [ -a_i, a_i ], \mbox{ and } \forall \mathbf r \in \prod_i [ -d_i, d_i ], \end{aligned} \end{align} 
  {\color{red} Not implemented}
  
  \section{Kalman filtered innovation models}
  \newcommand{\CA}[1]{\mathcal{#1}}
  \newcommand{\init}{\rho}
  
Consider a Gaussian LTI system:
 \begin{align}  \begin{aligned}
x_{k+1}&=A x_{k} + B u_t+ w_k,\\
z_k&=Cx_k+Du_k+v_k.\end{aligned} \end{align}
with $w_k\sim \mathcal N(0, \mathcal W)$ and $v_k\sim \mathcal N (0,\mathcal V)$.

At $k=0$, we know $x_0\sim \init$ with $\init:=\mathcal N(x_\init,P_\init)$.
Thus,  before receiving a measurement $z_0$, the distribution of the belief is defined as $\CA N(x_{0|-}, P_{0|-})$
\begin{align}
	\hat x_{0|-}&:= x_\init\\
	P_{0|-}&:= P_{\init}
\end{align}
After receiving the measurement $z_0$, this is updated to $\CA N(\hat x_{0|0}, P_{0|0})$
\begin{align}
	\hat x_{0|0}&:= x_\init+ L_0 (z_0-Cx_\init)\\
	P_{0|0}&:=(I-L_0 C) P_{\init}(I-L_0 C)^T+L_0\CA V L_0^T\\
	& \mbox{ with } L_0=P_{\init}C^T\left(CP_{\init}C^T+\mathcal V\right)^{-1}
\end{align}
We represent the belief state  $\CA N(\hat x_{0|0}, P_{0|0})$ by $b_0:=(\hat x_{0|0}, P_{0|0})\in\mathbb R^n\times \mathbb S^n$.

The dynamics of the Kalman filter are given as
	\begin{align*}
	&&\textbf{Predict} \qquad \hat x_{k|k-1}&=A\hat x_{k-1|k-1}+Bu_{k-1}\\
	&&P_{k|k-1}&=AP_{k-1|k-1}A^T+\mathcal W
\\
	&&\textbf{Update} \  \qquad e_{k}&=z_k-C \hat x_{k|k-1}\\
	&&S_k&=CP_{k|k-1}C^T+\mathcal V\\
	&&L_{k}&=P_{k|k-1}C^TS_k^{-1}\\
	&&\hat x_{k|k}&=\hat x_{k|k-1}+L_ke_k\\
	&&P_{k|k}&=(I-L_kC)P_{k|k-1}\\
	\end{align*}
	\mbox{Joseph Formula  }
	\begin{align*}
	&&P_{k|k}&=(I-L_kC)P_{k|k-1}(I-L_kC_k)^T+L_k\mathcal V_kL_k^T\\
		\end{align*}
\mbox{Observability based }
	\begin{align*}
	&& P_{k|k}^{-1}&=P_{k|k-1}^{-1}+C_k^T \mathcal  V_k^{-1}C_k
	\end{align*}

Though the covariance of the belief state is defined as 
	\begin{align*}
	&&P_{k|k}&=(I-L_kC)P_{k|k-1}(I-L_kC_k)^T+L_k\mathcal V_kL_k^T, \\
		\end{align*}
		The update equations for $P_{k|k-1}$ are more well know:
			\begin{align*}
	&&P_{k+1|k}&=(A-K_kC)P_{k|k-1}(A-K_kC_k)^T+K_k\mathcal V_kK_k^T  + \CA W
		\end{align*}
		with $K_k=AL_k$.
		
Hence, the belief state is updated as
\begin{align}
	&&\hat x_{k|k}&=A\hat x_{k-1|k-1}+Bu_{k-1}+L_ke_k\\
	&&P_{k|k}&=f(P_{k-1|k-1})
\end{align}
We now want to model the random variable $s_k=L_ke_k$. We know that $s_k$ evolves as a zero mean Gaussian distributed stochastic process.
Further 
\newcommand{\Ex}{\mathbf E}
\begin{align*}
	\Ex [s_k]=0\\
	\Ex [s_ks_k^T]=L_k	\Ex [e_ke_k^T]L_k^T, \mbox{ and } \Ex [e_ke_k^T]=S_k \\
	e_k = C\left(x_k- \hat x_{k|k-1}\right)+v_k\\
	\Ex [e_ke_k^T] = C P_{k|k-1} C^T + \CA V\\
		\Ex [s_ks_k^T]=L_k S_k L_k^T,\\
				\Ex [s_ks_k^T]= P_{k|k-1} C^T S_k^{-1} C P_{k|k-1},\\
								\Ex [s_ks_k^T]= P_{k|k-1} C^T \left(CP_{k|k-1}C^T+\mathcal V\right)^{-1} C P_{k|k-1},\notag\\
\Ex [s_ks_k^T]= P_{k|k-1}-P_{k|k}\notag
\end{align*}

\newcommand{\X}{\mathbb{X}}
Consider a LTI system
 \begin{align} \label{eq:LTI} \begin{aligned}
x_{k+1}&=A x_{k} + B u_k+ w_k\\
z_k&=Cx_k+Du_k+v_k\end{aligned} \end{align}
with $x\in \mathbb{R}^n$ with stochastic disturbances $w_t\sim \mathcal N(0,\CA W)$,  and $v_t\sim \mathcal N(0,\CA V)$. 
 \eqref{eq:LTI}  defines a MDP with state space $\X=\mathbb R^n$,  initial distribution  $\init:=\mathcal N(x_\init,P_\init)$,  control inputs $u_t\in\mathbb R^m$, and transition kernel $t$ defined based on  \eqref{eq:LTI}. This is a partially observable MDP that can only be observed via  $z_t\in\mathbb R^q$.
 
 \newcommand{\po}{\mathbb{P}}
%At $k=0$, we know $x_0\sim \init$ with $\init:=\mathcal N(x_\init,P_\init)$.
Before receiving a measurement $z_0$, the initial state is distributed  as $\CA N(x_{0|-}, P_{0|-})$, 	with $\hat x_{0|-}:= x_\init$ and $P_{0|-}:= P_{\init}$.
After receiving the measurement $z_0$, this is updated to \begin{align*}
	\hat x_{0|0}&:= x_\init+ L_0 (z_0-Cx_\init),\\
	P_{0|0}&:=(I-L_0 C) P_{\init}(I-L_0 C)^T+L_0\CA V L_0^T,\\
	& \mbox{ with } L_0=P_{\init}C^T\left(CP_{\init}C^T+\mathcal V\right)^{-1},
\end{align*}
with $\po(x_t\in \cdot\,|\,\rho,z_0):=\CA N(\hat x_{0|0}, P_{0|0})$.
This probability distribution defines a belief state as $b_0:=(\hat x_{0|0}, P_{0|0})\in\mathbb R^n\times \mathbb S^n$. The belief space $\X_b$ is  a finite dimensional space and can be parameterized. For example, let $\CA{G}$ denote the Gaussian belief space
    of dimension $n$, i.e. the space of Gaussian
    probability measures over $\mathbb{R}^n$.
    For brevity, we identify the Gaussian measures
    with their finite parametrization, mean and
    covariance matrix.
     Thus,
    $\X_b =  \mathbb{R}^n \times  \mathbb S^n$.


The dynamics of  $b_k:=(\hat x_{k|k}, P_{k|k})$ are defined via the 
 Kalman filter, that is
	\begin{align*}
	&\text{\it predict: }&\hat x_{k|k-1}&=A\hat x_{k-1|k-1}+Bu_{k-1} \\
	&&P_{k|k-1}&=AP_{k-1|k-1}A^T+\mathcal W,
\\
	%&&\textbf{Update} %\  \qquad e_{k}&=z_k-C \hat x_{k|k-1}\\
	%&&S_k&=CP_{k|k-1}C^T+\mathcal V\\
	%&&
	&\text{\it update: }&\hat x_{k|k}&=\hat x_{k|k-1}+L_k\left(z_k-C \hat x_{k|k-1}\right)\\
	&&P_{k|k}&=(I-L_kC)P_{k|k-1}
	\end{align*}
	with  $L_{k}=P_{k|k-1}C^T\left(CP_{k|k-1}C^T+\mathcal V\right)^{-1}$.
 
This defines a belief MDP  with stochastic transitions of the belief state given as 
\begin{align}
	&&\hat x_{k|k}&=A\hat x_{k-1|k-1}+Bu_{k-1}+P_{k|k-1}C^Ts_k\label{eq:beliefx}\\
	&&P_{k|k}&=f(P_{k-1|k-1})
\end{align}
with $e_k\sim \mathcal N (0, S_k^{-1})$ and  $S_k=\left(CP_{k|k-1}C^T+\mathcal V\right)$.
\newcommand{\MB}{\mathcal{B}}

 \newcommand{\grid}{d}
% We define an abstraction of $\MB{(\POMDP)}$  as $\hat\MB$ with state space $\mathbb R^n$ and stochastic transitions
As a first simplification, we can replace the stochastic transitions  in \eqref{eq:beliefx} by
\begin{align}  
		&&\hat x_k &=A\hat x_{k-1} +B\hat u_{k-1} + \bar P  C^T  \hat{s}_k,\label{eq:abstract} 
\end{align}
with $ \hat{s}_k\sim \CA N (0,\hat{S}_{inv})$ and $\hat{S}_{inv}\preceq S_k^{-1}$ for all $k$.\\ 

\medskip
\noindent\fbox{
\begin{minipage}[b]{.9\textwidth}
	The computational implementation is as follows: 
	\begin{align}
		\textbf{objective: } \min_{W\succeq0, s_{inv}} &\operatorname{trace}(W)\\
		%
		\mbox{s.t.  }\quad &W\succeq S_{k}^{-1}-S_{inv}\succeq0\label{eq:W}
	\end{align}
	And \eqref{eq:W} is equivalent to 
	\begin{align}
		&W+S_{inv} - S_{k}^{-1} \succeq 0, \quad S_{k}^{-1} -S_{inv}   \succeq 0\\
		&\begin{bmatrix}
			W+S_{inv} &I\\
			I & S_k
		\end{bmatrix}\succeq0, \quad \begin{bmatrix}
			S_{inv}^{-1}&  I\\
			 I& S_k ^{-1}
		\end{bmatrix}\succeq0
	\end{align}
	with $S_k=\left(CP_{k|k-1}C^T+\mathcal V\right)$
	\begin{align}
				&\begin{bmatrix}
			W+S_{inv} &I\\
			I & \left(CP_{k|k-1}C^T+\mathcal V\right)
		\end{bmatrix}\succeq0, \quad S_{inv}^{-1}- \left(CP_{k|k-1}C^T+\mathcal V\right)\succeq0
	\end{align}
	Given $P^-\preceq P_{k|k-1}\preceq P^+$
	\begin{align}
				&\begin{bmatrix}
			W+S_{inv} &I\\
			I & \left(CP^-C^T+\mathcal V\right)
		\end{bmatrix}\succeq0, \quad\left(CP^+C^T+\mathcal V\right)^{-1}- S_{inv} \succeq0
	\end{align}
Note 	$W\succeq  \left(CP^-C^T+\mathcal V\right)^{-1} -S_{inv}  $.
Hence the final solution is
\begin{align}
	W&=\left(CP^-C^T+\mathcal V\right)^{-1}-\left(CP^+C^T+\mathcal V\right)^{-1}\\
	S_{inv}&=\left(CP^+C^T+\mathcal V\right)^{-1}
\end{align}
	
	
	\end{minipage}}

These stochastic transitions \eqref{eq:abstract} can then be further abstracted to a finite state model $\hat\MB$ with states $s\in S=1,2,..., $. Each state $s$ is associated to a representative point $x_s\in \X_b$ and associated to a 
cell $\Delta_s=\{x_s\} \oplus \prod_n [-\grid, \grid]$.
Further the absstract system  has transitions 
\begin{align}\label{eq:tgrid}
t_{grid}(s'|s,u)=\hat t \left(\Delta_{s'}\mid x_s, u\right)
\end{align}where $\hat t$ is the stochastic transition kernel associated with \eqref{eq:abstract}.

\newcommand{\rel}{\mathcal{R}}
Consider a simulation relation defined as 
	\begin{align}\label{eq:rel}
\rel := \left\{(s,b_k)| (\hat x_{k|k}-x_s)^T M(\hat x_{k|k}-x_s)\leq \epsilon, \right.\\\qquad\left.  P^-\preceq P_{k|k} \preceq   P^+ \mbox{ with } b_k=(\hat x_{k|k}, P_{k|k} ) \right\},\notag
	\end{align}
and an interface 
\newcommand{\InF}{\mathcal{U}_v}
\[\InF(\hat u, \hat x, \hat x_{\,|\,}):=K( \hat x_{\,|\,} -\hat x)+\hat u\]
for some matrices $M, K,P^+,P^-$.

We can quantify the difference between $\MB$ and $\hat\MB$ via \eqref{eq:rel} by verifying that for all  $(\hat x_k,\hat x_{k|k})\in \rel$ with probability at least $1-\delta$ it holds that $(\hat x_{k+1},\hat x_{k+1|k+1})\in \rel$. 
Consider a choice for the lifted stochastic  transitions  for \eqref{eq:abstract} and \eqref{eq:beliefx2},  denoted 
	$ \mathbb W_{x}((\hat x_k, \hat x_{k|k})\in \cdot| \hat u_{k-1}, \hat x_{k-1}, \hat x_{k-1|k-1})$, based on the combined stochastic difference equation given as
\begin{align*}
		&&\hat x_{k+1} &=A\hat x_{k} +B\hat u_{k} + \bar P  C^T  \hat{s}_{k+1},\\%\label{eq:abstract3} \\
	&&\hat x_{k+1|k+1}&=A\hat x_{k|k}+Bu_{k}+  \bar P   C^T(  \hat{s}_{k+1}+s^\Delta_{k+1})\notag\\&&&\qquad+\Delta_{k+1}( \hat{s}_{k+1}+ s^\Delta_{k+1})%\label{eq:beliefx3}
\end{align*}
 with $\Delta_k:=(P_{k|k-1}C^T-  \bar P   C^T)$ and with $ \hat{s}_k\sim \CA N (0,\hat{S}_{inv})$ and $ s^\Delta_k\sim  \CA N (0,\  S_k^{-1}-\hat{S}_{inv}). $

We can now choose the lifted stochastic transition kernel 	$\mathbb W_t$ for the concrete belief MDP $\MB$ and the abstracted finite MDP $\hat\MB$ as follows.
Denote $b=(\hat x_{\,|\,}, P)$ and $b_+=(\hat x_{+\,|\,+}, P_+)$, then 	$\mathbb W_t$ is computed as 
 \begin{align*}
 &	\mathbb W_t((s_+,b_+)\in \cdot\,| \hat u, s ,b)\\&:= \left\{\begin{array}{ll} \mathbb W_{x}((\Delta_{s_+}, \hat x_{+|+})\in \cdot\,|  \hat u,x_s , \hat x_{\,|\,}) &\text{ for }  P_+=f(P)\\
 	0 & \text{ else } \end{array}\right.
 \end{align*}

For this choice of  	$\mathbb W_x$, the difference expression in \eqref{eq:rel} evolves   as 
\begin{align}
 \hat x_{k+1|k+1}-	\hat x_{k+1}=(A+BK)(\hat x_{k|k}-\hat x_{k-1})\qquad \quad\notag\\+  \bar P   C^T s^\Delta_{k+1} +\Delta_{k+1}( \hat{s}_{k+1}+ s^\Delta_{k+1})\label{eq:beliefx2}
\end{align}
 with $\Delta_{k+1}:=(P_{k+1|k}C^T-  \bar P   C^T)$, and with $ \hat{s}_{k+1}\sim \CA N (0,\hat{S}_{inv})$ and $ s^\Delta_{k+1}\sim  \CA N (0,\  S_{k+1}^{-1}-\hat{S}_{inv}). $
For all $ \hat x_{k+1}$, there exists $\mathbf  r \in \prod_n[-\grid ,\grid]$ such that   $\hat x_{k+1}-\mathbf r \in \{x_s| s \in S\}$. Therefore we can write the update of the difference expression as  \begin{align}
 \hat x_{+|+}-	\hat x_{s_+}=(A+BK)(\hat x_{\,|\,}-\hat x_s)+\mathbf r\qquad \quad\notag\\+  \bar P   C^T s^\Delta_{k+1} +\Delta_{k+1}( \hat{s}_{k+1}+ s^\Delta_{k+1})\label{eq:beliefx2}.
\end{align}
Given that $(\hat x_{\,|\,}-\hat x_s)$ and  $\mathbf r$ belongs to a bounded set, we can bound the influence of the noise terms $s^\Delta_{k+1}$ and $ \hat{s}_{k+1}$ with respect to a probability at least $1-\delta$ for which the update is always in $\rel$ cf.  \eqref{eq:rel}.\\
\fbox{\begin{minipage}[b]{\textwidth}
	\begin{align}
 \hat x_{+|+}-	\hat x_{s_+}=(A+BK)(\hat x_{\,|\,}-\hat x_s)+\mathbf r\qquad \quad\notag\\+  \bar P   C^T s^\Delta_{k+1}+\Delta_{k+1} s^\Delta_{k+1}+\Delta_{k+1} \hat{s}_{k+1} \label{eq:beliefx2}.
\end{align}
  We  want to find an upper bound for the random variable $\Delta_{k+1} \hat{s}_{k+1}$. This random variable has Gaussian distribution with covariance $\Delta_{k+1}  S_{inv}\Delta_{k+1} ^T.$ Hence we look for the minimal $S_\Delta$ {\color{red}(with respect to the trace (or determinant?) )}such that  $S_\Delta \succeq \Delta_{k+1}  S_{inv}\Delta_{k+1} ^T.$
  This is equivalent to
  \begin{align}
  S_\Delta -	\Delta_{k+1} S_{inv}\Delta_{k+1} ^T\succeq0\\
  \begin{bmatrix}
  	S_\Delta & (P_{k+1|k}-  \bar P )  C^T\\
  	C(P_{k+1|k} - \bar P )&\left(CP^+C^T+\mathcal V\right)
  \end{bmatrix}\succeq0
  \end{align}
  Write $P_{k+1|k} - \bar P$  as $H^+-H^-=P_{k+1|k} - \bar P$ with minimal matrices $H^+\succeq0$ and $H^-\succeq0$ (if $xH^+x>0$ then $xH^-x=0$, and if $xH^-x>0$ then $xH^+x=0$,).  Assume that $P^- \preceq  \bar P\preceq P^+$, then   based on $P^--\bar P\preceq P_{k|k-1}-\bar P\preceq P^+-\bar P$ it follows that $H^-\preceq\bar P-P^-$  and $ H^+\preceq P^+-\bar P$.
     \begin{align} 
  \begin{bmatrix}
  	S_\Delta & (H^+-H^- )  C^T\\
  	C(H^+-H^-)&\left(CP^+C^T+\mathcal V\right)
  \end{bmatrix}\succeq0\\
    \begin{bmatrix}
  	S_\Delta -H^+-H^-& 0\\
0&\left(CP^+C^T+\mathcal V\right)-C(H^++H^-)C^T
  \end{bmatrix}\\+  \begin{bmatrix}
  I \\
  	C   \end{bmatrix}H^+\begin{bmatrix}
  I \\
  	C   \end{bmatrix}^T+\begin{bmatrix}
  I \\
  	-C   \end{bmatrix}H^-\begin{bmatrix}
  I \\
  	-C   \end{bmatrix}^T\succeq0\end{align}
  We can see that $\left(CP^+C^T+\mathcal V\right)-C(H^++H^-)C^T\succeq0$ always holds. Therefore   	$S_\Delta -H^+-H^-\succeq 0$ is a sufficient condition.  Since  $xH^+x>0$ then $xH^-x=0$, and if $xH^-x>0$ then $xH^+x=0$, we design $S_\Delta$ to be minimal and such that $S_\Delta\succeq \bar P-P^- $ and $S_\Delta\succeq P^+-\bar P$.
   	
\end{minipage}}
 
 
 
Find an upper bound on the  the random variable $P_{k+1|k} C^Ts^{\Delta}_{k+1}$. This random variable has Gaussian distribution with covariance $ P_{k+1|k} C^T(S_k^{-1}-\hat{S}_{inv})CP_{k+1|k} .$ Hence we look for the minimal $S_\Delta$ {\color{red}(with respect to the trace (or determinant?) )}such that  $W_\Delta \succeq P_{k+1|k} C^TWCP_{k+1|k} .$
\begin{align}
	\begin{bmatrix}
		W_{\Delta} & P_{k+1|k} C^T\\
		CP_{k+1|k} &W^{-1}
	\end{bmatrix}\succeq 0\\\begin{bmatrix}
		W_{\Delta} & P_{k+1|k} C^TW\\
		WCP_{k+1|k} &W
	\end{bmatrix}\succeq 0
	\\\begin{bmatrix}
		W_{\Delta} + P_{k+1|k}& 0\\
		0&W+WCP_{k+1|k} C^TW
	\end{bmatrix}-
	\begin{bmatrix}
		I  \\
		-WC
	\end{bmatrix}P_{k+1|k} \begin{bmatrix}
		I  \\
		-WC
	\end{bmatrix}^T
	\succeq 0\end{align}A sufficient condition follows as \begin{align}
		\begin{bmatrix}
		W_{\Delta} + P^-& 0\\
		0&W+WCP^- C^TW
	\end{bmatrix}-
\begin{bmatrix}
		I  \\
		-WC
	\end{bmatrix}P^+\begin{bmatrix}
		I  \\
		-WC
	\end{bmatrix}^T
	\succeq 0\\
	\begin{bmatrix}
		W_{\Delta} + P^- -P^+ & P^+ C^TW\\
		WCP^+&W+WC(P^- -P^+)C^TW
	\end{bmatrix}
	\succeq 0
\end{align}
As an alternative  $W = \left(CP^-C^T+\mathcal V\right)^{-1}-\left(CP^+C^T+\mathcal V\right)^{-1}$ together with  $W_\Delta \succeq P_{k+1|k} C^TWCP_{k+1|k} $ gives
\begin{align}
	W_\Delta - P_{k+1|k} C^T\left(\left(CP^-C^T+\mathcal V\right)^{-1}-\left(CP^+C^T+\mathcal V\right)^{-1}\right) CP_{k+1|k} \succeq0\\
	W_\Delta+P_{k+1|k} C^T\left(CP^+C^T+\mathcal V\right)^{-1}  CP_{k+1|k} - P_{k+1|k} C^T\left(CP^-C^T+\mathcal V\right)^{-1}  CP_{k+1|k} \succeq0\\
	\begin{bmatrix}
		W_\Delta+P_{k+1|k} C^T\left(CP^+C^T+\mathcal V\right)^{-1}  CP_{k+1|k}&P_{k+1|k} C^T\\
		 CP_{k+1|k} &CP^-C^T+\mathcal V
	\end{bmatrix}\succeq0\\
		\begin{bmatrix}
		W_\Delta+P_{k+1|k} C^T\left(CP^+C^T+\mathcal V\right)^{-1}  CP_{k+1|k}-P_{k+1|k} &0%P_{k+1|k} C^T
		\\
		 0 %CP_{k+1|k}
		  & %CP^-C^T+
		  \mathcal V
	\end{bmatrix}+ \begin{bmatrix}
		I\\C
	\end{bmatrix}  P_{k+1|k}\begin{bmatrix}
		I\\C
	\end{bmatrix}\succeq0\\
		\begin{bmatrix}
		W_\Delta+P_{k+1|k} C^T\left(CP^+C^T+\mathcal V\right)^{-1}  CP_{k+1|k}-P_{k+1|k} &0%P_{k+1|k} C^T
		\\
		 0 %CP_{k+1|k}
		  & %CP^-C^T+
		  \mathcal V
	\end{bmatrix}+ \begin{bmatrix}
		I\\C
	\end{bmatrix}  P^-\begin{bmatrix}
		I\\C
	\end{bmatrix}\succeq0
\end{align}

Find $P_{k+1|k} C^T\left(CP^+C^T+\mathcal V\right)^{-1}  CP_{k+1|k} \succeq M$ {\color{red} go over $P_+$ instead of over $P_-$}
\begin{align}
	C^T\left(CP^+C^T+\mathcal V\right)^{-1}  C  -P_{k+1|k}  ^{-1}MP_{k+1|k}  ^{-1}\succeq 0\\
	\begin{bmatrix}
		C^T\left(CP^+C^T+\mathcal V\right)^{-1}  C & P_{k+1|k}  ^{-1}\\P_{k+1|k}  ^{-1}&M^{-1}
	\end{bmatrix}\succeq 0\\
		\begin{bmatrix}
		C^T\left(CP^+C^T+\mathcal V\right)^{-1}  C +P_{k+1|k}  ^{-1}& 0\\ 0&M^{-1}+P_{k+1|k}  ^{-1}
	\end{bmatrix}-\begin{bmatrix}
	I\\ -I
\end{bmatrix}P_{k+1|k}  ^{-1}\begin{bmatrix}
	I\\ -I
\end{bmatrix}^{T}
\succeq 0\\
		\begin{bmatrix}
		C^T\left(CP^+C^T+\mathcal V\right)^{-1}  C +P_{k+1|k}  ^{-1}& 0\\ 0&M^{-1}+P_{k+1|k}  ^{-1}
	\end{bmatrix}-\begin{bmatrix}
	I\\ -I
\end{bmatrix}P_{-}  ^{-1}\begin{bmatrix}
	I\\ -I
\end{bmatrix}^{T}
\succeq 0\\
		\begin{bmatrix}
		C^T\left(CP^+C^T+\mathcal V\right)^{-1}  C +P_{k+1|k}  ^{-1}-P_{-}  ^{-1}& P_{-}  ^{-1}\\ P_{-}  ^{-1}&M^{-1}+P_{k+1|k}  ^{-1}-P_{-}  ^{-1}
	\end{bmatrix} 
\succeq 0\\
\end{align}

 \begin{align}
 	\begin{bmatrix}
		W_{\Delta} & (\bar P + H^+ -H^- )C^TW\\
		WC(\bar P + H^+ -H^- )&W
	\end{bmatrix}\succeq 0\\
 	\begin{bmatrix}
		W_{\Delta}+H^++H^- & \bar P   C^TW\\
		WC\bar P &W+WCH^+C^TW+WCH^-C^TW
	\end{bmatrix}\qquad\\ -
\begin{bmatrix}
		I  \\
		-WC
	\end{bmatrix}H^+\begin{bmatrix}
		I  \\
		-WC
	\end{bmatrix}^T-\begin{bmatrix}
		I  \\
		 WC
	\end{bmatrix}H^-\begin{bmatrix}
		I  \\
		 WC
	\end{bmatrix}^T\succeq 0
 \end{align}
 
% 
%  \begin{align}
% \hat x_{+|+}-	\hat x_{s_+}=(A+BK)(\hat x_{\,|\,}-\hat x_s)+\mathbf r\qquad \quad\notag\\+  \bar P   C^T s^\Delta_{k+1} +\Delta_{k+1}( \hat{s}_{k+1}+ s^\Delta_{k+1})\label{eq:beliefx2}.
%\end{align}
%
%

 \section{Model-reduction + ...} 
 
  
 \chapter{Non-Gaussian systems}
 Consider the gridding of a stochastic process. 
 For non-gaussian systems it makes sense to use some of the well known measures between probability measures.

The {\bf total variation distance} $\delta(P,Q)$ is the most promising, 
\begin{align}
\delta(P,Q) =\sup\{|P(A)-Q(A) | A\in \Sigma \mbox{ is a measurable event.} \}	
\end{align}


Via Pinskers inequality we have that 
\begin{align}\delta(P,Q)\leq \sqrt{1/2 D_{KL}(P||Q)}.\end{align}
where the latter is the Kullback-Leibler divergence 
 

 
 \chapter{Shrinking and expanding polytopes}
 
 \section{Based on ellipsoid  sets}
 
 Consider polytope composed of the intersection of half spaces.
We develop the reasoning first in 3D and then expend it to multidimensional vectors.
 Given a plane defined as 
\[ax+by+cz=d,\]
this defines a half space as 
\[\{(x,y,z)\mid ax+by+bz\leq d\}.\]

Consider the case where $ a^2 + b^2 + c^2 =1$, then the vector $(a,b,c)$ defines the normal vector on the plane.
The plane is shifted with vector $r_0=(ad,bd,cd)$.
\[\{(x,y,z)\mid a(x-ad)+b(y-bd)+c(z-cd)\leq 0\}.\]
We can now shift it as follows $r_0$ as $(d+\epsilon)(a,b,c)$
 

In conclusion we can expand the half place with $\|(x,y,z)\|\leq \epsilon$
\[\{(x,y,z)\mid ax+by+bz\leq d+\epsilon\}.\]
Similarly, we can shrink it with $\|(x,y,z)\|\leq \epsilon$
\[\{(x,y,z)\mid ax+by+bz\leq d-\epsilon\}.\]
Remark that for this $\|(a,b,c)\|=1$.

To expand with   $r^T Mr  \leq \epsilon^2$, define $r = M^{-1/2}\tilde r$,  then $\tilde r = M^{1/2} r$.  By transforming the half space with $ M^{1/2}$, and normalize it we can do the above trick again.



\end{document}
